---
title: "Popularity Prediction"
subtitle: "New York Times Blog Articles"
author: "Philippe Lambot"
date: "May 12, 2020"

output: 
  html_document:
    toc: true
    toc_depth: 4
    number_sections: true
    toc_float: 
      collapsed: false
      smooth_scroll: false
    highlight: espresso
    df_print: paged
    theme: readable
    code_folding: hide
---

```{r Setup}
# Avoiding messages and warnings: anyway, they have already been dealt with.  
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

# Regulating figure layout.
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")

# Facilitating table layout in HTML.
options(knitr.table.format = "html")

# Output language
invisible(Sys.setlocale("LC_ALL", "C"))
```

<style type = "text/css">                  # Title colors
h1 {color: #08457E}
h2 {color: #08457E}
h3 {color: #08457E}
h4 {color: #08457E}
</style>

<style type = "text/css">                  # No bullets in TOC
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0;
}
</style>

<style type = "text/css">                  # Font size
<font size = "3">
</style>

<style type = "text/css">                  # Double text justification
body {
text-align: justify}
</style>

<br>

# EXECUTIVE SUMMARY

<br>

   

TAGS:  

<br>

GITHUB: https://github.com/Dev-P-L/NLP-News-Popularity-Prediction

<br>

## II. FOREWORD about DATA: ACKNOWLEDGEMENT

<br>

This research project is based on Data  

With the exception of labels used to represent categorical data, we have focused on numerical data, but in many applications data starts as text. Well known examples are spam filtering, cyber-crime prevention, counter-terrorism and sentiment analysis.

In all these examples, the raw data is composed of free form texts. Our task is to extract insights from these data. In this section, we learn how to generate useful numerical summaries from text data to which we can apply some of the powerful data visualization and analysis techniques we have learned.

During he 2016 US presidential election then candidate Donald J. Trump used his tweeter account as a way to communicate with potential voters. On August 6, 2016 Todd Vaziri tweeted about Trump that "Every non-hyperbolic tweet is from iPhone (his staff). Every hyperbolic tweet is from Android (from him)." Data scientist David Robison conducted an analysis to determine if data supported this assertion. Here we go through David's analysis to learn some of the basics of text mining.

This project is lodged with the GitHub repository https://github.com/Dev-P-L/NLP-News-Popularity-Prediction and is comprised of four files:

- README.md,
- NLP_News-Popularity-Prediction.Rmd (all code),
- NLP_News-Popularity-Prediction.html (methods, insights, results)
- and NLP_News-Popularity-Prediction.oxps (idem in other format).

<br>

```{r Cleaning up workspace and downloading packages}
# Cleaning up workspace for RAM management.
invisible(if(!is.null(dev.list())) dev.off())
rm(list=ls())
cat("\014")

# Downloading packages.
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(dslabs)) install.packages("dslabs", repos = "http://cran.us.r-project.org")
if(!require(utf8)) install.packages("utf8", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(scales)) install.packages("scales", repos = "http://cran.us.r-project.org")
if(!require(tm)) install.packages("tm", repos = "http://cran.us.r-project.org")
if(!require(wordcloud)) install.packages("wordcloud", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(tidytext)) install.packages("tidytext", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")

# Requiring libraries.
library(tidyverse)
library(dslabs)
library(utf8)
library(lubridate)
library(scales)
library(tm)
library(wordcloud)
library(kableExtra)
library(gridExtra)
library(caret)
library(pROC)
library(ggthemes)
library(tidytext)
library(stringr)
```

<br>

## III. DATA PREPARATION

... in the dslabs package:

```{r Downloading data}
data("trump_tweets")
tweets <- trump_tweets
```

This is data frame with information about the tweet.
Documentation available at ?trump_tweets.
The variables that are included are

```{r Getting in touch with data}
str(tweets, vec.len = 1)
```

The tweets are represented by the text variable:

The source variable tells us the device that was used to compose and upload each tweet.

```{r Variable source}
tweets %>% count(source) %>% arrange(desc(n))
```

There are

- a little bit than 8,000 tweets,
- a little bit more than 4,500 tweets from Android,
- almost 4,000 from iPhone.

We can use extract to remove the Twitter for part of the source and filter out retweets.

Number of retweets = TRUE per source is zero for all sources. 

```{r Number of retweets}
df <- tweets %>% select(source, is_retweet) %>% 
  group_by(source) %>%
  summarize(nr_retweet = length(is_retweet[is_retweet == TRUE]))
df
rm(df)
```

Extracting tweets whose source name begins with "Twitter for". Usefulness of it?

```{r Extracting tweets}
df <- trump_tweets %>% 
  extract(source, "source", "Twitter for (.*)") %>%
  count(source) 
df
rm(df)
```

We are interested in what happened during the campaign, so for the analysis here we will focus on what was tweeted between the day Trump announced his campaign and election day. So we define the following table:

```{r Extracting only two devices}
elect_tweets <- tweets %>% 
  mutate(device = str_replace(str_replace(source, "Twitter for Android",                    "Android"), "Twitter for iPhone", "iPhone")) %>%
  filter(device %in% c("Android", "iPhone") &
         created_at >= ymd("2015-06-17") & 
         created_at < ymd("2016-11-08")) %>%
  mutate(am_pm = gsub('[0-9: ]+','\\1',format(created_at, '%r'))) %>%
  arrange(created_at) %>%
  select(- source) %>%
  select(device, everything()) 
```

We can now use data visualization to explore the possibility that two different groups were tweeting from these devices. For each tweet, we will extract the hour, in the east coast (EST), it was tweeted then compute the proportion of tweets tweeted at each hour for each device.

In pie charts.

```{r Graph am_pm 24 hour-clock PIE CHART}
# The palette with grey:
cbbPalette <- c("#E69F00", "#0072B2", "#000000", "#56B4E9", 
                "#009E73", "#F0E442",  "#D55E00", "#CC79A7")

graph <- elect_tweets %>%
  mutate(hour = hour(with_tz(created_at, "EST"))) %>%
  select(device, am_pm) %>%
  group_by(device, am_pm) %>%
  summarise(n = n()) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(device, percent, fill = am_pm)) +
  geom_bar(width = 0.5, stat = "identity") +
  scale_y_continuous(labels = percent_format(suffix = " %")) +
  labs(x = "",
       y = "% of Tweets per Device",
       fill = "Parts of the Day") +
  theme_economist(base_size = 12) +
  scale_fill_manual(values = cbbPalette) +
  theme(panel.background = element_rect(fill = "#56B4E9"))
graph
```

Now in hour.

```{r Graph am_pm 24 hour-clock}
# The palette with grey:
# Orange, blue, black, light blue, green, yellow, vermilion, reddish purple
cbbPalette <- c("#E69F00", "#0072B2", "#000000", "#56B4E9", 
                "#009E73", "#F0E442",  "#D55E00", "#CC79A7")

graph <- elect_tweets %>%
  mutate(hour = hour(with_tz(created_at, "EST"))) %>%
  select(device, hour) %>%
  group_by(device, hour) %>%
  summarise(n = n()) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(hour, percent, color = device)) +
  geom_point(aes(size = percent)) +
  scale_y_continuous(labels = 
                     percent_format(accuracy = 1, suffix = " %")) +
  labs(x = "Hour of day (EST)",
       y = "% of tweets",
       color = "") +
  theme_economist(base_size = 12) +
  scale_colour_manual(values = cbbPalette) +
  theme(panel.background = element_rect(fill = "#56B4E9"))
graph
```

We notice a big peak for the Android in early hours of the morning, between 6 and 8 AM. There seems to be a clear different in these patterns. We will therefore assume that two different entities are using these two devices. 

It is like adding apples and pears: day schedules are amalgamated over more than one year. Would find back that pattern sytematically in individual day schedules? Let's take the busiest 12 days in terms of number of tweets. 

Let's check up this assumption without amalgamating all days, which can be misleading. 

<br>

```{r Adapting opts_chunk to enlarge display width for a set of graphs}
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

```{r Graph hour for busy days}
buffer <- elect_tweets %>% 
  mutate(day = floor_date(with_tz(created_at, "EST"), unit = "day")) %>%
  mutate(hour = floor_date(with_tz(created_at, "EST"), unit = "hour"))

sample <- buffer %>% 
     group_by(day) %>%
     summarize(n = n()) %>%
     arrange(desc(n)) %>%
     head(12) %>%
     arrange(day) 

v <- sample %>% .$day

percentage <- 
  label_percent(accuracy = 0.1, suffix = " %")(sum(sample$n) / nrow(buffer))

l <- list(1) 

for (i in 1:length(v)) {
  d <- v[i]
  
  graph <- buffer %>%
    filter(day == d) %>% 
    select(device, hour) %>%
    group_by(device, hour) %>%
    summarise(n = n()) %>%
    ggplot(aes(hour, n, color = device)) +
    geom_point(aes(size = n)) +
    ggtitle(paste("Busy Day", i, ":", d, sep = " ")) +
    labs(x = "",
         y = "# of Tweets", 
         color = "") +
  theme_economist() +
  scale_colour_manual(values = cbbPalette) +
  theme(panel.background = element_rect(fill = "#56B4E9")) 
  
  l[[i]] <- graph
}  

marrangeGrob(l, nrow = 1, ncol = 1, 
  top = "")
```

```{r Adapting opts_chunk to enlarge display width for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

Provisional conclusion
######################

Activity by the two devices is intermingled in the sense that there is no clear-cut separation; the busiest hour periods differ clearly on average, but on a daily basis the busiest hour periods do not systematically differ. 

So, there are partial time leads and lags between devices. Would that be compatible with the hypothesis of two separate entities each using  one device? Actually, beyond the factual statement of these leads and lags, anything else is sheer speculation on the basis of information available. Moreover, the approach in this project is not at all person-related. Whether the two devices are or are not operated by the same entity or by two different entities is not material to our purposes. 

The factual statement of leads and lags is used in this EDA section to characterize tweets from the two devices. In machine learning, this piece of information can be used to attribute tweets to one particular device. 

-----------------------------------------------------------------------------

```{r Graph % AM per month for Android}
graph <- elect_tweets %>%
  mutate(month = round_date(with_tz(created_at, "EST"), unit = "month")) %>%
  select(device, month, am_pm) %>%
  filter(device == "Android") %>%
  group_by(month, am_pm) %>%
  summarize(n = n()) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(month, percent, fill = am_pm)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = percent_format(suffix = " %")) +
  labs(x = "",
       y = "% of Tweets per Device",
       fill = "Parts of the Day") +
  theme_economist(base_size = 12) +
  scale_fill_manual(values = cbbPalette) +
  theme(panel.background = element_rect(fill = "#56B4E9")) 
graph
```

```{r Graph % AM per month for iPhone}
graph <- elect_tweets %>%
  mutate(month = round_date(with_tz(created_at, "EST"), unit = "month")) %>%
  select(device, month, am_pm) %>%
  filter(device == "iPhone") %>%
  group_by(month, am_pm) %>%
  summarize(n = n()) %>%
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(month, percent, fill = am_pm)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = percent_format(suffix = " %")) +
  labs(x = "",
       y = "% of Tweets per Device",
       fill = "Parts of the Day") +
  theme_economist(base_size = 12) +
  scale_fill_manual(values = cbbPalette) +
  theme(panel.background = element_rect(fill = "#56B4E9")) 
graph
```

---------------------------------------------------------------------------------

EVOLUTION OF TWEET PUBLICATION

```{r Graph Tweet publication count}
graph <- elect_tweets %>%
  mutate(week = round_date(with_tz(created_at, "EST"), unit = "week")) %>%
  select(device, week) %>%
  group_by(device, week) %>%
  summarize(n = n()) %>%
  ggplot(aes(week, n, color = device)) +
  geom_line(size = 2) +
  labs(x = "",
       y = "% of Tweets per Device",
       color = "") +
  theme_economist(base_size = 12) +
  scale_colour_manual(values = cbbPalette) +
  theme(panel.background = element_rect(fill = "#56B4E9")) 
graph
```

-------------------------------------------------------------------

```{r Graph retweet_count}
graph <- elect_tweets %>%
  mutate(month = floor_date(with_tz(created_at, "EST"), unit = "month")) %>%
  select(device, month, retweet_count) %>%
  group_by(device, month) %>%
  summarize(avg = mean(n())) %>%
  ggplot(aes(month, avg, color = device)) +
  geom_line(size = 2) +
  labs(x = "",
       y = "% of Tweets per Device",
       fill = "Parts of the Day") +
  theme_economist(base_size = 12) +
  scale_color_manual(values = cbbPalette) +
  theme(panel.background = element_rect(fill = "#56B4E9")) 
graph
```

Now we will study how their tweets differ. 

NLP 

Text as data

The tidytext package helps us convert from text into a tidy table. Having the data in this format greatly facilitates data visualization and applying statistical techniques.

The main function needed to achieve this is unnest_tokens. A token refers to the units that we are considering to be a data point. The most common tokens will be words, but they can also be single characters, ngrams, sentences, lines or patterns defined by a regex. The functions will take a vector of strings and extract the tokens so that each one gets a row in the new table. 

```{r Exercise}
v <- data.frame(text = elect_tweets$text) 
v %>% unnest_tokens(word, text)

pattern <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
elect_tweets[i,] %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
  select(word)
```

Note that the function tries to convert tokens into words and strips characters important to twitter such as # and @. A token in twitter is not the same as in regular english. For this reason instead of using the default, words, we define a regex that captures twitter character. The pattern appears complex but all we are defining is a patter that starts with @, # or neither and is followed by any combination of letter or digits:

pattern <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"

We can now use the unnest_tokens function with the regex option and appropriately extract the hashtags and mentions:

campaign_tweets[i,] %>% 
  unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
  select(word)

Another minor adjustment we want to make is remove the links to pictures:

campaign_tweets[i,] %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
  select(word)

Now we are now read to extract the words for all our tweets.

tweet_words <- campaign_tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, text, token = "regex", pattern = pattern) 

And we can now answer questions such as "what are the most commonly used words?"

tweet_words %>% 
  count(word) %>%
  arrange(desc(n))

It is not surprising that these are the top words. The top words are not informative. The tidytext package has database of these commonly used words, referred to as stop words, in text mining:

stop_words

If we filter out rows representing stop words with filter(!word %in% stop_words$word)

tweet_words <- campaign_tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
  filter(!word %in% stop_words$word ) 

we end up with a much more informative set of top 10 tweeted words

tweet_words %>% 
  count(word) %>%
  top_n(10, n) %>%
  mutate(word = reorder(word, n)) %>%
  arrange(desc(n))

Some exploration of the resulting words (not show here) reveals a couple of unwanted characteristics in our tokens. First, some of our tokens are just numbers (years for example). We want to remove these and we can find them using the regex ^\d+$. Second, some of our tokens come from a quote and they start with '. We want to remove the ' when it's at the start of a word so we will use str_replace. We add these two lines to the code above to generate are final table:

tweet_words <- campaign_tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
  unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
  filter(!word %in% stop_words$word &
           !str_detect(word, "^\\d+$")) %>%
  mutate(word = str_replace(word, "^'", ""))

Now that we have all our words in a table, along with information about what device was used to compose the tweet they came from, we can start exploring which words are more common when comparing Android to iPhone.

For each word we want to know if it is more likely to come from an Android tweet or an iPhone tweet. We previously introduced the odds ratio a summary statistic useful for quantifying these differences. We each device and a given word, let's call it y, we compute the odds or the ratio between the proportion of words that are y and not y and compute the ratio of those odds. Here we will have many proportions that are 0 so we use the 0.5 correction.

android_iphone_or <- tweet_words %>%
  count(word, source) %>%
  spread(source, n, fill = 0) %>%
  mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) / 
           ( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5)))
android_iphone_or %>% arrange(desc(or))
android_iphone_or %>% arrange(or)

Given that several of these words are overall low frequency words we can impose a filter based on the total frequency like this:

android_iphone_or %>% filter(Android+iPhone > 100) %>%
  arrange(desc(or))

android_iphone_or %>% filter(Android+iPhone > 100) %>%
  arrange(or)

We already see somewhat of a pattern in the types of words that are being tweeted more in one device versus the other. However, we are not interested in specific words but rather in the tone. Vaziri's assertion is that the Android tweets are more hyperbolic. So how can we check this with data? Hyperbolic is a hard sentiment to extract from words as it relies on interpreting phrases. However, words can be associated to more basic sentiment such as as anger, fear, joy and surprise. In the next section we demonstrate basic sentiment analysis.

Sentiment Analysis

In sentiment analysis we assign a word to one or more "sentiment". Although this approach will miss context dependent sentiments, such as sarcasm, when performed on large numbers of words, summaries can provide insights.

The first step in sentiment analysis is to assign a sentiment to each word. The tidytext package includes several maps or lexicons in the object sentiments:

table(sentiments$lexicon)

The bing lexicon divides words into positive and negative. We can see this using the tidytext function get_sentiments:

get_sentiments("bing")

The AFINN lexicon assigns a score between -5 and 5, with -5 the most negative and 5 the most positive.

get_sentiments("afinn")

The loughran and nrc lexicons provide several different sentiments:

get_sentiments("loughran") %>% count(sentiment)
get_sentiments("nrc") %>% count(sentiment)

To start learning about how these lexicons were developed read this help
file ?sentiments.

For the analysis here we are interested in exploring the different sentiments of each tweet so we will use the nrc lexicon:

nrc <- sentiments %>%
  filter(lexicon == "nrc") %>%
  select(word, sentiment)

We can combine the words and sentiments using inner_join, which will only keep words associated with a sentiment. Here are 10 random words extracted from the tweets:

tweet_words %>% inner_join(nrc, by = "word") %>% 
  select(source, word, sentiment) %>% sample_n(10)

Now we are ready to perform a quantitative analysis comparing Android and iPhone by comparing the sentiments of the tweets posted from each device. Here we could perform a tweet by tweet analysis, assigning a sentiment to each tweet. However, this somewhat complex since each tweet will have several sentiments attached to it, one for each word appearing in the lexicon. For illustrative purposes we will perform a much simpler analysis: we will count and compare the frequencies of each sentiment appears for each device.

sentiment_counts <- tweet_words %>%
  left_join(nrc, by = "word") %>%
  count(source, sentiment) %>%
  spread(source, n) %>%
  mutate(sentiment = replace_na(sentiment, replace = "none"))
sentiment_counts

Because more words were used on the Android than on the phone:

tweet_words %>% group_by(source) %>% summarize(n = n())

For each sentiment we can compute the odds of being in the device: proportion of words with sentiment versus proportion of words without and then compute the odds ratio comparing the two devices

sentiment_counts %>%
  mutate(Android = Android / (sum(Android) - Android) , 
         iPhone = iPhone / (sum(iPhone) - iPhone), 
         or = Android/iPhone) %>%
  arrange(desc(or))

So we do see some difference and the order is interesting: the largest three sentiments are disgust, anger, and negative! But are they statistically significant? How does this compare if we are just assigning sentiments at random?

To answer that question we can compute, for each sentiment, an odds ratio and confidence interval. We will add the two values we need to form a two-by-two table and the odd rat

library(broom)
log_or <- sentiment_counts %>%
  mutate( log_or = log( (Android / (sum(Android) - Android)) / (iPhone / (sum(iPhone) - iPhone))),
          se = sqrt( 1/Android + 1/(sum(Android) - Android) + 1/iPhone + 1/(sum(iPhone) - iPhone)),
          conf.low = log_or - qnorm(0.975)*se,
          conf.high = log_or + qnorm(0.975)*se) %>%
  arrange(desc(log_or))
  
log_or

A graphical visualization shows some sentiments that are clearly overrepresented:

log_or %>%
  mutate(sentiment = reorder(sentiment, log_or),) %>%
  ggplot(aes(x = sentiment, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar() +
  geom_point(aes(sentiment, log_or)) +
  ylab("Log odds ratio for association between Android and sentiment") +
  coord_flip() 

We see that the disgust, anger, negative sadness and fear sentiments are associated with the Android in a way that is hard to explain by chance alone. Words not associated to a sentiment were strongly associated with the iPhone source, whic is in agreement with the original claim about hyperbolic tweets.

If we are interested in exploring which specific words are driving these differences, we can back to our android_iphone_or object:

android_iphone_or %>% inner_join(nrc) %>%
  filter(sentiment == "disgust" & Android + iPhone > 10) %>%
  arrange(desc(or))

We can make a graph

android_iphone_or %>% inner_join(nrc, by = "word") %>%
  mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) %>%
  mutate(log_or = log(or)) %>%
  filter(Android + iPhone > 10 & abs(log_or)>1) %>%
  mutate(word = reorder(word, log_or)) %>%
  ggplot(aes(word, log_or, fill = log_or < 0)) +
  facet_wrap(~sentiment, scales = "free_x", nrow = 2) + 
  geom_bar(stat="identity", show.legend = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 



------------------------------------------------------------------

COLORS

The first two references deal with color-blind-friendly issues. 

http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/

https://venngage.com/blog/color-blind-friendly-palette/#2

http://www.sthda.com/english/wiki/ggplot2-themes-and-background-colors-the-3-elements

---------------------------------------------------------------------

tidytext package
https://www.tidytextmining.com/


janeaustenr package !!!!
https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html


REGEX
https://bookdown.org/rdpeng/rprogdatascience/regular-expressions.html


NLP

https://cran.r-project.org/web/views/NaturalLanguageProcessing.html

https://stackoverflow.com/questions/21533899/in-r-use-gsub-to-remove-all-punctuation-except-period/39745610
The \\1 is syntax for the last capture in a regular expression using the () 
It says whatever was matched, replace it with that. 
I put only the "." and "-" in the group (), so \\1 will replace .- 
(by the same vale), so it keeps them here. â€“ agstudy Feb 3 '14 at 20:02

Extracting hashtags
https://stackoverflow.com/questions/13762868/how-do-i-extract-hashtags-from-tweets-in-r
There are two levels of parsing going on here. 
Before the low level regexp function within str_extract 
gets the pattern you want to search for (i.e. "#\S+") 
it is first parsed by R. R does not recognize \S 
as a valid escape character and throws an error. 
By escaping the slash with \\ you tell R to pass the \ and S 
as two normal characters to the regexp function, 
instead of interpreting it as one escape character.
v <- str_extract_all(train_tweets$text, "#\\S+")

SPLITTING BETWEEN LOWER CASE AND UPPER CASE
https://stackoverflow.com/questions/43706474/splitting-string-between-capital-and-lowercase-character-in-r
We can use regex lookaround to match lower case letters 
(positive lookbehind - (?<=[a-z])) followed by upper case letters 
(positive lookahead -(?=[A-Z]))

Not used but ...
https://cran.r-project.org/web/packages/textclean/textclean.pdf

---

https://blog.datazar.com/first-debate-2016-sentimental-analysis-of-candidates-58d87092fc6a
twitter authentication
web scraping
sentiment analysis
simplistic in comments?

https://www.kaggle.com/erikbruin/text-mining-the-clinton-and-trump-election-tweets
awesome!

---------------------------------------------------------------------

STUDY ITSELF - BOOK FROM RAF

https://books.google.be/books?id=62K-DwAAQBAJ&pg=PA459&lpg=PA459&dq=str_replace_all(text,+https://t.co/%5BA-Za-z%5C%5Cd%5D%2B%7C%26amp;,+)&source=bl&ots=bDWhUw2slL&sig=ACfU3U3Kvhh-parwC0mYLod1HL_yvlWLnQ&hl=en&sa=X&ved=2ahUKEwiGycS7pNHpAhXElqQKHR7RDvcQ6AEwAHoECAoQAQ#v=onepage&q=str_replace_all(text%2C%20https%3A%2F%2Ft.co%2F%5BA-Za-z%5C%5Cd%5D%2B%7C%26amp%3B%2C%20)&f=false


WORDCLOUDS

Retrieving, unnesting and buidling up wordclouds

https://www.littlemissdata.com/blog/wordclouds

---

Focused on wordclouds, with Twitter logo!

https://cran.r-project.org/web/packages/wordcloud2/vignettes/wordcloud.html

---

Focused on wordclouds

https://www.r-graph-gallery.com/196-the-wordcloud2-library.html

---

Wordclouds and DRACULA gutenbergr

https://www.learningrfordatascience.com/post/dynamic-wordclouds-with-wordcloud2/



